---
title: "p8105_hw3_xx2416"
author: "Xicheng Xie"
date: "2022-10-10"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#load packages
```{r}
library(tidyverse)
library(patchwork)
library(ggridges)
knitr::opts_chunk$set(
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1
### data import
```{r}
library(p8105.datasets)
data("instacart")
```
This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Key variables of this dataset include `aisle` and `aisle_id`, which denote the name and identifier of the aisle, `product_id` and `product_name`, which denote the identifier and name of the product, `department_id` and `department`, which denote the identifier and name of the department, `order_dow` and `order_hour_of_day`, which denote the the day of the week and the hour of the day on which the order was placed, `days_since_prior_order`, which denotes days since the last order. Other variables not involved are out of interest for other research purposes.
### illstrative examples of observations
For example, in the first 8 consecutive rows, there are eight products bought by user #112108 in his/her 4th purchase at 10 am on Thursday, 9 days since his/her last order. The fist product is Bulgarian Yogurt in the **yogurt** aisle of the **dairy eggs** department.
```{r}
instacart[1:8,]
```


### Qustion 1
How many aisles are there, and which aisles are the most items ordered from?
```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarise(n_obs=n()) %>% 
  arrange(desc(n_obs))

```
There are `r instacart %>% distinct(aisle) %>% nrow()` different aisles in this dataset. `Fresh vegetables` is the aisle in which the most items order from (150609 items), `fresh fruits` is the second most frequently ordered aisle with 150473 items ordered. 

### Question 2
Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

To make the plot more readable, I make a point and a bar plot respectively, and use `patchwork` to present the two plots side by side. 
```{r}
aisle_point_plot<-
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs=n()) %>% 
  filter(n_obs>10000) %>% 
  mutate(aisle=fct_reorder(aisle,n_obs,.desc = TRUE)) %>% 
  ggplot(aes(x=aisle,y=n_obs))+
  geom_point(alpha = .5)+
  labs(
    title="Number of items ordered in each aisle (>10000)",
    y="Number of items",
    caption = "data from instacart"
  )+
  theme(axis.text.x = element_text(angle = 90,vjust=0.5 ,hjust = 1))+
  scale_y_continuous(breaks = seq(10000,200000,by=30000),limits = c(10000,160000))

aisle_col_plot<-
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs=n()) %>% 
  filter(n_obs>10000) %>% 
  mutate(aisle=fct_reorder(aisle,n_obs,.desc = TRUE)) %>% 
  ggplot(aes(x=aisle,y=n_obs))+
  geom_col()+
  coord_flip()+
  labs(
    title="Number of items ordered in each aisle (>10000)",
    y="Number of items",
    x="Asile",
    caption = "data from instacart"
  )+
  theme(legend.position = "bottom")

aisle_point_plot/aisle_col_plot
```

### Question 3
Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r}
instacart %>% 
  group_by(aisle,product_name) %>%
  filter(aisle %in% c("baking ingredients", "dog food care","packaged vegetables fruits")) %>% 
  summarize(n_obs=n()) %>% 
  mutate(times_ranking=min_rank(desc(n_obs))) %>% 
  filter(times_ranking<4) %>% 
  select(aisle,product_name,n_obs) %>% 
  arrange(desc(n_obs)) %>% 
  knitr::kable(
    col.names = c("Aisle","Top 3 popular items","Number of orders"),
    align = "llc",
    caption = "Top 3 popular items from certain aisles")

```
As intuitively presented in the table, 
* In the `packaged vegetables fruits` aisle, the top 3 popular items are `Organic Baby Spinach` with 9784 order times, `Organic Raspberries` with 5546 order times, and `Organic Blueberries` with 4966 order times.
* In the `baking ingredients` aisle, the top 3 popular items are `Light Brown Sugar` with 499 order times, `Pure Baking Soda` with 387 order times, and `	Cane Sugar` with 336 order times. 
* In the `dog food care` aisle, the top 3 popular items are `Snack Sticks Chicken & Rice Recipe Dog Treats`, `Organix Chicken & Brown Rice Recipe`, and `Small Dog Biscuits`, with 30, 28, and 26 order times respectively.

### Question 4 
Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>%
  mutate(order_dow=recode_factor(order_dow,
    `0`="Sun",`1`="Mon",`2`="Tue",`3`="Wed",`4`="Thu",`5`="Fri",`6`="Sat"
  )) %>% 
  group_by(product_name,order_dow) %>%
  summarize(mean_orhd=mean(order_hour_of_day)) %>%
  select(product_name,order_dow,mean_orhd) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_orhd
  ) %>% 
  knitr::kable(digits = 2)
  
```

# Problem 2

### Question 1
Load, tidy, and otherwise wrangle the data. 
```{r}
accel_df<-
  read_csv("accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day=factor(day,levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    day_kind=case_when(
    day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday")~"weekday",
    day %in% c("Sunday","Saturday")~"weekend"),
    day_kind=factor(day_kind,levels = c("weekday","weekend"))) %>%
  pivot_longer(
    starts_with("activity_"),
    names_to = "activity.*",
    names_prefix = "activity_",
    values_to = "activity_counts"
    )
```
In the tidied new dataset `accel_df`, there are `r nrow(accel_df)` observations, and key variables include `week`,`day_id`,`day`, which is converted to factor variable, `day_kind`, which denote weekday or weekend of the observation day, `activity.*` denote the activity counts for each minute of a 24-hour day starting at midnight, and `activity_counts` variable contains the counting value.

### Question 2
Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?
```{r}
accel_df %>% 
  group_by(week,day) %>% 
  summarize(total_activity=sum(activity_counts)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
  knitr::kable(digits = 2,
               align = 'lccccccc',
               caption = "Total activity over each day in each week"
  )
```

The table of total activity on each day is shown above. To better discover the trends, two maps are made and presented.
```{r}
total_trend1<-
  accel_df %>% 
  group_by(week,day) %>% 
  summarize(total_activity=sum(activity_counts)) %>% 
  ggplot(aes(x=day,y=total_activity,color=week))+
  geom_point()+
  geom_line(aes(group=week))+
  theme(axis.text.x = element_text(angle = 90,vjust=0.5 ,hjust = 1))

total_trend2<-
  accel_df %>% 
  group_by(day_id) %>% 
  summarize(total_activity=sum(activity_counts)) %>% 
  ggplot(aes(x=day_id,y=total_activity))+
  geom_line()

total_trend1+total_trend2
```
Actually, I don't find any trends showed based on the data above.

### Question 3
Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. 
```{r}
accel_df %>%
  mutate(`activity.*` = as.integer(`activity.*`)) %>% 
  ggplot(aes(x=`activity.*`,y=activity_counts,group=day_id,color=day))+
  geom_point()+
  geom_line()+
  geom_smooth()+
  scale_x_continuous(
    breaks = (0:24)*60,
    labels = 0:24
  )+
  labs(
    x="hour of each day",
    title = "24-hour activity time courses for each day"
  )

```

# Problem 3

### data import
```{r}
library(p8105.datasets)
data("ny_noaa")
```
### do some exploration of this dataset
In the dataset `ny_noaa`, there are `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The key variables of this df include `id`, Weather station ID, `date`, Date of observation, `prcp`, Precipitation (tenths of mm), `snow`, Snowfall (mm), `snwd`, Snow depth (mm), `tmax`, Maximum temperature (tenths of degrees C), and `tmin`, Minimum temperature (tenths of degrees C). 
There are `r ny_noaa %>% distinct(id) %>% count()` different weather stations. Since each weather station may collect only a subset of these variables, the resulting dataset contains extensive missing data.

### Missing data analysis
```{r}
ny_noaa %>% 
  janitor::clean_names() %>%
  summarise_all(funs(100*mean(is.na(.)))) %>% 
  knitr::kable(digits = 2,
               caption="Proportion of missing values for all variables")
```

The missing value proportion of all the variables is shown in the table above. `tmax` and `tmin` contain high percentage of missing values, which are 43.71%, then following `snwd` and `snow`. The high percentage of missing values in the variables will undoubtedly affect the stability of the data analysis results.

### Do some data cleaning
Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.
```{r}
ny_noaa_df<-
  ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date,into = c("year","month","day"),sep = '-',convert = TRUE) %>% 
  mutate(
    month=month.abb[month],
    prcp=prcp/10,
    tmax=as.numeric(tmax)/10,
    tmin=as.numeric(tmin)/10
  ) %>% 
  select(id,year,month,day,everything())
```

About snowfall, the most commonly observed value showed below is 0, which means there is no snow in NY most of the time. This make sense, since the climate of NY is generally humid continental. Winter temperatures average below freezing during January and February in much of the state of New York, but several degrees above freezing along the Atlantic coastline, including New York City.
```{r}
ny_noaa_df %>% 
 count(snow) %>%
  arrange(desc(n))
```
### Question 2
Make a two-panel plot showing the average **max** temperature in January and in July in each station across years. 
```{r}
ny_noaa_df %>% 
  filter(month %in% c("Jan","Jul")) %>%
  group_by(id,year,month) %>%
  summarize(mean_tmax=mean(tmax,na.rm=TRUE)) %>% 
  ggplot(aes(x=year,y=mean_tmax))+
  geom_smooth(color="red",size=2)+
  geom_point(size=0.5,alpha=0.2)+
  geom_line(aes(group=id,color=id),alpha=0.3)+
  facet_grid(.~month)+
  scale_x_continuous(breaks = seq(1980,2010,by = 5))+
  labs(
    y="Average max temperature of the month (degrees C)",
    title = "Average max temperature in January and in July in each station across year",
    caption= "Data from ny_noaa"
  )+
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5),legend.position = "none")
```

From the plot above, we can find that the average max temperature in January fluctuates between -10 degree C and 10 degree C, and the average max temperature in July fluctuates between 20 and 35 degree C. The varies of average tmax tends to be bigger in Jan, and there are a few outliers in both months.

### Question 3
Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
tmax_tmin_plot<-
  ny_noaa_df%>% 
  ggplot(aes(x=tmax,y=tmin))+
  geom_hex()+
  geom_smooth(se=FALSE,na.rm = TRUE)
  labs(
    x="tmax (degree C)",
    y="tmin (degree C)",
    title = "tmax vs tmin for the full dataset",
    caption="Data from ny_noaa"
  )


snow_density_plot<-
  ny_noaa_df %>% 
  filter(snow>0 & snow<100) %>% 
  mutate(year=as.factor(year)) %>% 
  ggplot(aes(x=snow, y=year,fill=year))+
  geom_density_ridges(rel_min_height = 0.005,scale=2.5)+
  labs(
    y="Year",
    x="Snowfall (mm)",
    title = "The distribution of snowfall values (0~100) separately by year",
    caption= "Data from ny_noaa")+
  theme_ridges()+theme(legend.position = "none")

tmax_tmin_plot+snow_density_plot

```














